# 生成式AI

## ChatGPT

- <https://chat.openai.com>

## Claude

- 「ChatGPT最强竞品」爆火：不限量不要钱免注册！一手实测体验在此: <https://www.qbitai.com/2023/04/46508.html>


## AI大模型应用开发

- 应用层: ChatGPT、 NewBing、Github Copilot、NotionAI、Midjourney
- 中间件层: LangChain
- 基础模型层: GPT4、Bert

### 2023.07.12 大模型基础：理论与技术的演进
- AI发展的四个阶段：Artificial Intelligence -> Machine Learning -> Deep Learning -> Large Language Model
- 注意力机制(Attention Mechanism)
  - 高频 ！= 重点
  - Encoder-decoder Architecture with Attention Model
- 变革里程碑：(Transformer架构) 论文（Attention is all you need）
- GPT、BERT属于两个不同的方向
- 论文
  - Seq2Seq模型
    - sequence to sequence learning with neural networks：最基本的 Seq2Seq 模型，encoder-decoder 架构
    - Learnin g Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation
  - 注意力机制:
    - Neural Machine Translation by Jointly Learning to Align and Translate (2014) - Bahdanau等首次提出注意力机制- Effective Approaches to Attention-based Neural Machine Translation (2015) - Luong等提出全局注意力和局部注意力
  - Transformer:
    - Attention Is All You Need (2017) - Vaswani等人提出仅基于注意力机制的Transformer
  - GPT系列:
    - Improving Language Understanding by Generative Pre-Training (2018) - Radford等提出GPT语言模型  
    - Language Models are Unsupervised Multitask Learners (2019) - Radford等提出GPT-2
    - Devlin等提出BERT- Language Models are Few-Shot Learners (2020) - Brown等提出GPT-3
  - BERT系列:
    - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018) 
    


#### 其他学习资源分享
- 动手学深度学习: <https://zh.d2l.ai/index.html>
